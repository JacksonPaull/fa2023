{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_networkx\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import itertools\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv\n",
    "import dgl.function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('..')\n",
    "\n",
    "import src.synthetic as synthetic\n",
    "import src.transform as transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split_edge(data):\n",
    "    # Create a list of positive and negative edges\n",
    "    u, v = data.edges()\n",
    "    u, v = u.numpy(), v.numpy()\n",
    "    edge_index = np.array((u, v))\n",
    "    adj = coo_matrix((np.ones(data.num_edges()), edge_index))\n",
    "    adj_neg = 1 - adj.todense() - np.eye(data.num_nodes())\n",
    "    neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "    # Create train/test edge split\n",
    "    test_size = int(np.floor(data.num_edges() * 0.1))\n",
    "    eids = np.random.permutation(np.arange(data.num_edges())) # Create an array of 'edge IDs'\n",
    "\n",
    "    train_pos_u, train_pos_v = edge_index[:, eids[test_size:]]\n",
    "    test_pos_u, test_pos_v   = edge_index[:, eids[:test_size]]\n",
    "\n",
    "    # Sample an equal amount of negative edges from  the graph, split into train/test\n",
    "    neg_eids = np.random.choice(len(neg_u), data.num_edges())\n",
    "    test_neg_u, test_neg_v = (\n",
    "        neg_u[neg_eids[:test_size]],\n",
    "        neg_v[neg_eids[:test_size]],\n",
    "    )\n",
    "    train_neg_u, train_neg_v = (\n",
    "        neg_u[neg_eids[test_size:]],\n",
    "        neg_v[neg_eids[test_size:]],\n",
    "    )\n",
    "\n",
    "    # Remove test edges from original graph\n",
    "    train_g = deepcopy(data)\n",
    "    train_g.remove_edges(eids[:test_size]) # Remove positive edges from the testing set from the network\n",
    "\n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=data.num_nodes())\n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=data.num_nodes())\n",
    "\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=data.num_nodes())\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=data.num_nodes())\n",
    "\n",
    "    return train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    )\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    ).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, \"mean\")\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "\n",
    "class DotPredictor(torch.nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata[\"score\"][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(G):\n",
    "    # TODO Work on getting this to be more feature agnostic - i.e. take the join of all this stuff and null if not present\n",
    "    # Also need a stored one-hot \n",
    "\n",
    "    # Change type to two features, is_student, and is_org\n",
    "    G_eng = deepcopy(G)\n",
    "    _type = np.asarray(list(nx.get_node_attributes(G_eng, 'type').items()))\n",
    "    is_student = np.asarray(_type[:,1] == 'student', dtype='float32')\n",
    "    # commitment_limit = list(nx.get_node_attributes(G, 'commitment_limit').values())\n",
    "\n",
    "    X = np.column_stack([is_student, 1-is_student])\n",
    "    nx.set_node_attributes(G_eng, dict(zip(_type[:,0], X)), 'X')\n",
    "\n",
    "    # TODO Add major in as one-hot\n",
    "\n",
    "    # TODO Add Year in as one-hot\n",
    "\n",
    "\n",
    "    return G_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = synthetic.synthesize_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_eng = engineer_features(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.from_networkx(G_eng, node_attrs=['X']) # TODO Investigate the slowness here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=315, num_edges=958,\n",
       "      ndata_schemes={'X': Scheme(shape=(2,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g = create_train_test_split_edge(G)\n",
    "\n",
    "model = GraphSAGE(train_g.ndata[\"X\"].shape[1], 32)\n",
    "pred = DotPredictor()\n",
    "optimizer = torch.optim.Adam(\n",
    "    itertools.chain(model.parameters(), pred.parameters()), lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 11.222268104553223\n",
      "AUC 0.07357340720221606\n",
      "In epoch 5, loss: 1.07867431640625\n",
      "In epoch 10, loss: 1.1495293378829956\n",
      "In epoch 15, loss: 0.961980938911438\n",
      "In epoch 20, loss: 0.6747779250144958\n",
      "In epoch 25, loss: 0.5894042253494263\n",
      "In epoch 30, loss: 0.5782065391540527\n",
      "In epoch 35, loss: 0.5644545555114746\n",
      "In epoch 40, loss: 0.5414692163467407\n",
      "In epoch 45, loss: 0.5274854302406311\n",
      "In epoch 50, loss: 0.5222107768058777\n",
      "In epoch 55, loss: 0.5185007452964783\n",
      "In epoch 60, loss: 0.5156992673873901\n",
      "In epoch 65, loss: 0.5136780142784119\n",
      "In epoch 70, loss: 0.5121491551399231\n",
      "In epoch 75, loss: 0.5113844275474548\n",
      "In epoch 80, loss: 0.5108682513237\n",
      "In epoch 85, loss: 0.510433554649353\n",
      "In epoch 90, loss: 0.5101290345191956\n",
      "In epoch 95, loss: 0.5099584460258484\n",
      "In epoch 100, loss: 0.5098488926887512\n",
      "AUC 0.9401108033240997\n",
      "In epoch 105, loss: 0.5097494125366211\n",
      "In epoch 110, loss: 0.5096747875213623\n",
      "In epoch 115, loss: 0.5096328854560852\n",
      "In epoch 120, loss: 0.5095926523208618\n",
      "In epoch 125, loss: 0.5095564126968384\n",
      "In epoch 130, loss: 0.5095258951187134\n",
      "In epoch 135, loss: 0.5094974637031555\n",
      "In epoch 140, loss: 0.5094695687294006\n",
      "In epoch 145, loss: 0.5094422101974487\n",
      "In epoch 150, loss: 0.5094155669212341\n",
      "In epoch 155, loss: 0.5093888640403748\n",
      "In epoch 160, loss: 0.5093621611595154\n",
      "In epoch 165, loss: 0.5093357563018799\n",
      "In epoch 170, loss: 0.5093092918395996\n",
      "In epoch 175, loss: 0.5092829465866089\n",
      "In epoch 180, loss: 0.5092566013336182\n",
      "In epoch 185, loss: 0.509230375289917\n",
      "In epoch 190, loss: 0.5092043280601501\n",
      "In epoch 195, loss: 0.5091782808303833\n",
      "In epoch 200, loss: 0.509152352809906\n",
      "AUC 0.9398337950138504\n",
      "In epoch 205, loss: 0.5091264843940735\n",
      "In epoch 210, loss: 0.5091007351875305\n",
      "In epoch 215, loss: 0.5090751647949219\n",
      "In epoch 220, loss: 0.509049654006958\n",
      "In epoch 225, loss: 0.5090241432189941\n",
      "In epoch 230, loss: 0.5089987516403198\n",
      "In epoch 235, loss: 0.5089734196662903\n",
      "In epoch 240, loss: 0.5089483261108398\n",
      "In epoch 245, loss: 0.5089231133460999\n",
      "In epoch 250, loss: 0.508898138999939\n",
      "In epoch 255, loss: 0.5088730454444885\n",
      "In epoch 260, loss: 0.5088481307029724\n",
      "In epoch 265, loss: 0.5088233351707458\n",
      "In epoch 270, loss: 0.5087985396385193\n",
      "In epoch 275, loss: 0.5087738037109375\n",
      "In epoch 280, loss: 0.5087490677833557\n",
      "In epoch 285, loss: 0.5087243914604187\n",
      "In epoch 290, loss: 0.5086997151374817\n",
      "In epoch 295, loss: 0.5086750388145447\n",
      "In epoch 300, loss: 0.5086504220962524\n",
      "AUC 0.9391689750692522\n",
      "In epoch 305, loss: 0.5086274147033691\n",
      "In epoch 310, loss: 0.5086031556129456\n",
      "In epoch 315, loss: 0.5085798501968384\n",
      "In epoch 320, loss: 0.5085558891296387\n",
      "In epoch 325, loss: 0.5085315704345703\n",
      "In epoch 330, loss: 0.5085069537162781\n",
      "In epoch 335, loss: 0.5084823369979858\n",
      "In epoch 340, loss: 0.508457601070404\n",
      "In epoch 345, loss: 0.5084326267242432\n",
      "In epoch 350, loss: 0.5084074139595032\n",
      "In epoch 355, loss: 0.508382260799408\n",
      "In epoch 360, loss: 0.5083568096160889\n",
      "In epoch 365, loss: 0.5083320736885071\n",
      "In epoch 370, loss: 0.5083088278770447\n",
      "In epoch 375, loss: 0.5082844495773315\n",
      "In epoch 380, loss: 0.5082594752311707\n",
      "In epoch 385, loss: 0.508233904838562\n",
      "In epoch 390, loss: 0.5082077980041504\n",
      "In epoch 395, loss: 0.5081813335418701\n",
      "In epoch 400, loss: 0.5081546306610107\n",
      "AUC 0.9408864265927978\n",
      "In epoch 405, loss: 0.5081296563148499\n",
      "In epoch 410, loss: 0.5081055760383606\n",
      "In epoch 415, loss: 0.5080800652503967\n",
      "In epoch 420, loss: 0.5080533027648926\n",
      "In epoch 425, loss: 0.5080259442329407\n",
      "In epoch 430, loss: 0.5079978704452515\n",
      "In epoch 435, loss: 0.507969319820404\n",
      "In epoch 440, loss: 0.5079418420791626\n",
      "In epoch 445, loss: 0.5079144239425659\n",
      "In epoch 450, loss: 0.507885754108429\n",
      "In epoch 455, loss: 0.5078579187393188\n",
      "In epoch 460, loss: 0.5078294277191162\n",
      "In epoch 465, loss: 0.5077996850013733\n",
      "In epoch 470, loss: 0.5077727437019348\n",
      "In epoch 475, loss: 0.5077450275421143\n",
      "In epoch 480, loss: 0.5077153444290161\n",
      "In epoch 485, loss: 0.5076842308044434\n",
      "In epoch 490, loss: 0.5076519846916199\n",
      "In epoch 495, loss: 0.5076206922531128\n",
      "In epoch 500, loss: 0.5075899362564087\n",
      "AUC 0.941218836565097\n",
      "In epoch 505, loss: 0.5075576305389404\n",
      "In epoch 510, loss: 0.5075259208679199\n",
      "In epoch 515, loss: 0.5074939131736755\n",
      "In epoch 520, loss: 0.5074614882469177\n",
      "In epoch 525, loss: 0.507428765296936\n",
      "In epoch 530, loss: 0.5073994398117065\n",
      "In epoch 535, loss: 0.5073674321174622\n",
      "In epoch 540, loss: 0.5073360800743103\n",
      "In epoch 545, loss: 0.5073015093803406\n",
      "In epoch 550, loss: 0.5072652101516724\n",
      "In epoch 555, loss: 0.5072274208068848\n",
      "In epoch 560, loss: 0.507195234298706\n",
      "In epoch 565, loss: 0.507164478302002\n",
      "In epoch 570, loss: 0.5071305632591248\n",
      "In epoch 575, loss: 0.5070937871932983\n",
      "In epoch 580, loss: 0.5070549845695496\n",
      "In epoch 585, loss: 0.5070176124572754\n",
      "In epoch 590, loss: 0.5069854259490967\n",
      "In epoch 595, loss: 0.5069490075111389\n",
      "In epoch 600, loss: 0.5069100856781006\n",
      "AUC 0.9404432132963989\n",
      "In epoch 605, loss: 0.5068743228912354\n",
      "In epoch 610, loss: 0.5068373680114746\n",
      "In epoch 615, loss: 0.5068003535270691\n",
      "In epoch 620, loss: 0.5067629218101501\n",
      "In epoch 625, loss: 0.5067291259765625\n",
      "In epoch 630, loss: 0.5066927671432495\n",
      "In epoch 635, loss: 0.5066531896591187\n",
      "In epoch 640, loss: 0.5066150426864624\n",
      "In epoch 645, loss: 0.5065773129463196\n",
      "In epoch 650, loss: 0.5065392851829529\n",
      "In epoch 655, loss: 0.5065018534660339\n",
      "In epoch 660, loss: 0.5064653158187866\n",
      "In epoch 665, loss: 0.5064266920089722\n",
      "In epoch 670, loss: 0.5063912868499756\n",
      "In epoch 675, loss: 0.5063534379005432\n",
      "In epoch 680, loss: 0.5063132047653198\n",
      "In epoch 685, loss: 0.5062758326530457\n",
      "In epoch 690, loss: 0.5062432885169983\n",
      "In epoch 695, loss: 0.506208598613739\n",
      "In epoch 700, loss: 0.5061696171760559\n",
      "AUC 0.9399445983379502\n",
      "In epoch 705, loss: 0.5061284899711609\n",
      "In epoch 710, loss: 0.5060897469520569\n",
      "In epoch 715, loss: 0.5060526728630066\n",
      "In epoch 720, loss: 0.5060175657272339\n",
      "In epoch 725, loss: 0.5059818029403687\n",
      "In epoch 730, loss: 0.5059444308280945\n",
      "In epoch 735, loss: 0.5059093832969666\n",
      "In epoch 740, loss: 0.5058711171150208\n",
      "In epoch 745, loss: 0.5058350563049316\n",
      "In epoch 750, loss: 0.5057997107505798\n",
      "In epoch 755, loss: 0.5057648420333862\n",
      "In epoch 760, loss: 0.5057288408279419\n",
      "In epoch 765, loss: 0.5057002305984497\n",
      "In epoch 770, loss: 0.5056688189506531\n",
      "In epoch 775, loss: 0.505631685256958\n",
      "In epoch 780, loss: 0.5055935382843018\n",
      "In epoch 785, loss: 0.5055673718452454\n",
      "In epoch 790, loss: 0.5055350661277771\n",
      "In epoch 795, loss: 0.5055007934570312\n",
      "In epoch 800, loss: 0.5054652094841003\n",
      "AUC 0.9413296398891967\n",
      "In epoch 805, loss: 0.5054372549057007\n",
      "In epoch 810, loss: 0.5054066777229309\n",
      "In epoch 815, loss: 0.5053774118423462\n",
      "In epoch 820, loss: 0.505344569683075\n",
      "In epoch 825, loss: 0.5053088068962097\n",
      "In epoch 830, loss: 0.5052781701087952\n",
      "In epoch 835, loss: 0.5052478909492493\n",
      "In epoch 840, loss: 0.5052239894866943\n",
      "In epoch 845, loss: 0.5051947236061096\n",
      "In epoch 850, loss: 0.5051627159118652\n",
      "In epoch 855, loss: 0.5051315426826477\n",
      "In epoch 860, loss: 0.505108118057251\n",
      "In epoch 865, loss: 0.5050826668739319\n",
      "In epoch 870, loss: 0.5050542950630188\n",
      "In epoch 875, loss: 0.5050236582756042\n",
      "In epoch 880, loss: 0.5050051808357239\n",
      "In epoch 885, loss: 0.5049824118614197\n",
      "In epoch 890, loss: 0.5049543380737305\n",
      "In epoch 895, loss: 0.5049251914024353\n",
      "In epoch 900, loss: 0.5048960447311401\n",
      "AUC 0.9385595567867036\n",
      "In epoch 905, loss: 0.5048668384552002\n",
      "In epoch 910, loss: 0.5048508644104004\n",
      "In epoch 915, loss: 0.5048270225524902\n",
      "In epoch 920, loss: 0.5048026442527771\n",
      "In epoch 925, loss: 0.5047770142555237\n",
      "In epoch 930, loss: 0.5047504305839539\n",
      "In epoch 935, loss: 0.5047232508659363\n",
      "In epoch 940, loss: 0.5046981573104858\n",
      "In epoch 945, loss: 0.5046741962432861\n",
      "In epoch 950, loss: 0.5046502947807312\n",
      "In epoch 955, loss: 0.504630446434021\n",
      "In epoch 960, loss: 0.5046082735061646\n",
      "In epoch 965, loss: 0.5045843720436096\n",
      "In epoch 970, loss: 0.5045595765113831\n",
      "In epoch 975, loss: 0.5045498609542847\n",
      "In epoch 980, loss: 0.5045285820960999\n",
      "In epoch 985, loss: 0.504504919052124\n",
      "In epoch 990, loss: 0.5044803023338318\n",
      "In epoch 995, loss: 0.50445556640625\n",
      "In epoch 1000, loss: 0.5044310688972473\n",
      "AUC 0.9413850415512465\n"
     ]
    }
   ],
   "source": [
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(1001):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata[\"X\"])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "    # ----------- 5. check results ------------------------ #\n",
    "    if e % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            pos_score = pred(test_pos_g, h)\n",
    "            neg_score = pred(test_neg_g, h)\n",
    "            print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
